{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from numpy import expand_dims\n",
    "from numpy import reshape\n",
    "from numpy import load\n",
    "from numpy import max\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "import dlib\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model = load_model('faceRecognition Model/facenet_keras.h5')\n",
    "with open('faceRecognition Model/svm_model.pkl', 'rb') as file:\n",
    "    model_svm = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_hog(image):\n",
    "    pixels = np.asarray(image)\n",
    "    # print(\"Pixels shape:\", pixels.shape)\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    dets = detector(pixels, 1)\n",
    "    # print(\"Detections:\", dets)\n",
    "\n",
    "    if len(dets) == 0:\n",
    "        return None\n",
    "\n",
    "    x1_face, y1_face, x2_face, y2_face = dets[0].left(), dets[0].top(), dets[0].right(), dets[0].bottom()\n",
    "\n",
    "    print(\"Face coordinates (x1, y1, x2, y2):\", x1_face, y1_face, x2_face, y2_face)\n",
    "\n",
    "    # Check if the detected face coordinates are within the image boundaries\n",
    "    if x1_face > 0:\n",
    "        x1_face = x1_face\n",
    "    else: \n",
    "        x1_face = 0\n",
    "\n",
    "    # y1_face = max(0, y1_face)\n",
    "    if y1_face > 0:\n",
    "        y1_face = y1_face\n",
    "    else:\n",
    "        y1_face = 0\n",
    "\n",
    "    # x2_face = min(pixels.shape[1], x2_face)\n",
    "    if x2_face < pixels.shape[1]:\n",
    "        x2_face = x2_face\n",
    "    else:\n",
    "        x2_face = pixels.shape[1]\n",
    "    \n",
    "    # y2_face = min(pixels.shape[0], y2_face)\n",
    "    if y2_face < pixels.shape[0]:\n",
    "        y2_face = y2_face\n",
    "    else:\n",
    "        y2_face = pixels.shape[0]\n",
    "\n",
    "\n",
    "    print(\"Cropped face coordinates (x1, y1, x2, y2):\", x1_face, y1_face, x2_face, y2_face)\n",
    "\n",
    "    if x1_face >= x2_face or y1_face >= y2_face:\n",
    "        return None\n",
    "\n",
    "    store_face = pixels[y1_face:y2_face, x1_face:x2_face]\n",
    "    image1 = Image.fromarray(store_face, 'RGB')   \n",
    "    image1 = image1.resize((160, 160))            \n",
    "    face_array = np.asarray(image1)              \n",
    "\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model,face_pixels):\n",
    "  face_pixels = face_pixels.astype('float32') \n",
    "  mean = face_pixels.mean()                   \n",
    "  std  = face_pixels.std()                     \n",
    "  face_pixels = (face_pixels - mean)/std\n",
    "  samples = np.expand_dims(face_pixels,axis=0)   \n",
    "  yhat = model.predict(samples)\n",
    "  return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_encode = LabelEncoder()\n",
    "persons = ['Abdelghafour', 'Ania', 'chris_evans', 'chris_hemsworth', 'Karima', 'mark_ruffalo', 'miko', 'robert_downey_jr', 'scarlett_johansson']\n",
    "out_encode.fit(persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_dict = {i: name for i, name in enumerate(persons)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Cropped face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Predicted class index: Ania\n",
      "Face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Cropped face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Predicted class index: Ania\n",
      "Face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Cropped face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Predicted class index: Ania\n",
      "Face coordinates (x1, y1, x2, y2): 253 168 382 297\n",
      "Cropped face coordinates (x1, y1, x2, y2): 253 168 382 297\n",
      "Predicted class index: Ania\n",
      "Face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Cropped face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Predicted class index: Ania\n",
      "Face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Cropped face coordinates (x1, y1, x2, y2): 259 175 366 283\n",
      "Predicted class index: Ania\n",
      "Face coordinates (x1, y1, x2, y2): 270 163 378 271\n",
      "Cropped face coordinates (x1, y1, x2, y2): 270 163 378 271\n",
      "Predicted class index: Ania\n",
      "Face coordinates (x1, y1, x2, y2): 247 163 354 271\n",
      "Cropped face coordinates (x1, y1, x2, y2): 247 163 354 271\n",
      "Predicted class index: mark_ruffalo\n",
      "Face coordinates (x1, y1, x2, y2): 235 175 342 283\n",
      "Cropped face coordinates (x1, y1, x2, y2): 235 175 342 283\n",
      "Predicted class index: Ania\n",
      "Face coordinates (x1, y1, x2, y2): 235 175 342 283\n",
      "Cropped face coordinates (x1, y1, x2, y2): 235 175 342 283\n",
      "Predicted class index: Ania\n",
      "Face coordinates (x1, y1, x2, y2): 247 175 354 283\n",
      "Cropped face coordinates (x1, y1, x2, y2): 247 175 354 283\n",
      "Predicted class index: Ania\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    face = detect_faces_hog(rgb_frame)\n",
    "\n",
    "    if face is not None:\n",
    "        face = face.reshape(160, 160, 3)\n",
    "\n",
    "        embeddings = extract_embeddings(face_model, face)\n",
    "\n",
    "        in_encode = Normalizer(norm='l2')\n",
    "        flattened_embeddings = embeddings.reshape(1, -1)\n",
    "        normalized_embeddings = in_encode.transform(flattened_embeddings)\n",
    "\n",
    "        predict_test = model_svm.predict(normalized_embeddings)\n",
    "        probability = model_svm.predict_proba(normalized_embeddings)\n",
    "        confidence = np.max(probability)\n",
    "\n",
    "        predicted_class_index = out_encode.inverse_transform(predict_test)[0]\n",
    "        print(\"Predicted class index:\", predicted_class_index)\n",
    "        predicted_class_key = f'class_{predicted_class_index}'\n",
    "\n",
    "\n",
    "        if confidence > 0.3:\n",
    "            cv2.putText(frame, f'Student: {predicted_class_index}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "             cv2.putText(frame, 'Prediction: Unknown', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):   \n",
    "        break\n",
    "\n",
    "camera.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nouwara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
